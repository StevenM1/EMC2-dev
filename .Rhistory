constants=c(s=log(1),st0=log(0),DP=qnorm(0.5),SZ=qnorm(0),sv=log(0))
# The DDM model is specified with this argument:
model=ddmTZD
# ddmTZD is a function exported by the EMC2 package. The function is for
# internal use so you don't need to understand its structure at this stage, but
# if you are curious you can look at it.
ddmTZD
# Two further arguments provide information about the data to be fit (in
# contrast to lm and glm the data are not supplied at this stage). The first,
# Ffactors (factors that can be used in the formuli) specifies a list of the
# levels of the factors spanning all of the cells in the data frame to be
# analyzed (i.e., E, CI and S)  which can have any name except for the names
# "R", "rt" and "subjects", which are reserved for columns in the data frame
# giving the response factor (R), a numeric response  time column (rt) and a
# factor column specifying the subjects, with levels  that must also be
# specified in the Ffactors argument. In this case there is  only one subject
# but later we will look at analyses of many subjects together.
Ffactors=list(subjects=levels(dat$subjects),
S=levels(dat$S),CI=levels(dat$CI),E=levels(dat$E))
# Like lm and glm make_design uses the linear modeling language, but requires
# a list of formulas, one for each model-parameter type, given as a list to the
# Flist (formula list) argument. We will start with a simple parameterizaiton
# where thresholds (a) are affected by E, rates (v) by S and we estimates a
# non-decision time (t0) and response bias (Z) that is the same for all
# conditions. Note that we have to provide formulas for all 9 of the DDM's
# parameter types, including the 5 not estimated, but these are ignored in
# estimation, being always set at the constant values specified above.
Flist=list(a~E,v~S,Z~1,t0~1,sv~1,st0~1, s~1, SZ~1, DP~1)
# Putting it all together
design <- make_design(model=model,
Ffactors=Ffactors,Rlevels=Rlevels,Flist=Flist,constants=constants)
levels(dat$R)
# We also separately specify the levels of the response factor in the Rlevels
# (response levels) argument. In more advanced applications this gives EMC2 the
# ability to move beyond binary choice. Note that levels can also be specified
# as character vectors.
Rlevels=levels(dat$R)
# Putting it all together
design <- make_design(model=model,
Ffactors=Ffactors,Rlevels=Rlevels,Flist=Flist,constants=constants)
# When your run this command it prints out the parameter vector that will be
# sampled. You can also extract this to an object in order to set values of this
# vector in order to simulate data. Here we see we have a 6 parameter model.
p_vector <- sampled_p_vector(design); p_vector[1:length(p_vector)]
# Similarly for a we see the intercept (a) is the threshold for the accuracy
# condition and a_Espeed the difference between speed and accuracy thresholds.
cbind(levels(dat$E),attr(p_vector,"map")$a)
# Lets now set these values and see how they map to the full design.
p_vector["v"] <- -1       # rate for left stimulus, mapped to lower boundary
p_vector["v_Sright"] <- 2 # right rate = -1 + 2 = 1, mapped to upper boundary
# To understand the remaining parameters we need to know that EMC2 always does
# sampling in an unbounded space. However, the natural scale for some parameters
# is not unbounded. For example a and t0 must be positive. Hence, EMC2 estimates
# log(a) and log(t0), which means that a and t0 must be positive (e.g., when
# converting back to natural scale by exponentiating, negative values are mapped
# to the range 0-1). Hence, if we want to set a=2 and t0 = 0.3
p_vector["a"] <- log(2)
p_vector["t0"] <- log(.3)
# Working with logs also has implications for how design matrices work. Adding
# logs is the same as multiplying on the natural scale. Suppose we want the
# speed condition to have half the threshold of the accuracy condition:
p_vector["a_Espeed"] <- log(0.5)
# Finally response bias Z, the position of the starting-point relative to the
# upper threshold must be confined to 0-1, so is estimated as a probit. Suppose
# we want to specify unbiased responding (e.g., Z = 0.5 i.e, half way between
# the lower and upper boundaries):
p_vector["Z"] <- qnorm(0.5)
# In summary out p_vector is:
p_vector[1:length(p_vector)]
# We can see how it maps t the design using this function (here we remove the
# constants for clarity). We see the values are as desired. Note that the
# output also includes the absolute bias z = a*Z, so 1 when a=2 and 0.5 when a=1.
mapped_par(p_vector,design)[,c(1:7,9,12,15)]
# Next lets simulate lots of data from this model and p_vector. Here n specifies
# the number of trials per design cell, so given 8 cells, 80,000 data points.
data <- make_data(p_vector,design,trials=10000); dim(data)
# The only difference in columns from out actual data is a trials column (which
# is not required in the data you analyse, although it can be present)
head(data)
# We also see its factors have the same levels.
lapply(data,levels)
# Greater accuracy and slower RTs in accuracy emphasis (88% and 0.8s) vs.
# speed emphasis (73% and 0.47s) are evident. Correct and error RT are the same
# (up to sampling error) as must be the case for the Wiener diffusion.
plot_defective_density(data,layout=c(2,2))
# However, we know that incongruent is slower and less accurate than congruent,
# lets model that with drift rates (i.e., a lower rate in incongruent). To do
# this we use two new features of the linear model language, removing the
# intercept, "0+", which allows us to have separate terms for the left and right
# drift rates, and "nesting", S/CI ("CI nested within S"), which allows us to
# estimate an additive term for left incongruent stimuli and right incongruent
# stimuli.FlistCI=list(a~E,v~0+S/CI,Z~1,t0~1,sv~1,st0~1, s~1, SZ~1, DP~1)
designCI <- make_design(model=model,
Ffactors=Ffactors,Rlevels=Rlevels,Flist=FlistCI,constants=constants)
# However, we know that incongruent is slower and less accurate than congruent,
# lets model that with drift rates (i.e., a lower rate in incongruent). To do
# this we use two new features of the linear model language, removing the
# intercept, "0+", which allows us to have separate terms for the left and right
# drift rates, and "nesting", S/CI ("CI nested within S"), which allows us to
# estimate an additive term for left incongruent stimuli and right incongruent
# stimuli.
FlistCI=list(a~E,v~0+S/CI,Z~1,t0~1,sv~1,st0~1, s~1, SZ~1, DP~1)
designCI <- make_design(model=model,
Ffactors=Ffactors,Rlevels=Rlevels,Flist=FlistCI,constants=constants)
p_vectorCI <- sampled_p_vector(designCI); p_vectorCI[1:length(p_vectorCI)]
attr(p_vectorCI,"map")$v
# v_Sleft and v_Sright are the rates for left and right congruent stimuli. Lets
# set these to the same as before.
p_vectorCI["v_Sleft"] <- -1
p_vectorCI["v_Sright"] <- 1
# v_Sleft:CIincongurent and v_Sright:CIincongurent are added to these rates to
# give the corresponding incongruent rates.
p_vectorCI["v_Sleft:CIincongurent"] <- 1/4    # incongruent left rate = -3/4
p_vectorCI["v_Sright:CIincongurent"] <- -1/4  # incongruent right rate = 3/4
# set the rest as before and simulate data
p_vectorCI[c("a","a_Espeed","Z","t0")] <- p_vector[c("a","a_Espeed","Z","t0")]
p_vectorCI[1:length(p_vectorCI)]
# We see that parameters are mapped as desired
mapped_par(p_vectorCI,designCI)[,c(1:7,9,12,15)]
# Now simulate lots of data and plot it
dataCI <- make_data(p_vectorCI,designCI,trials=10000)
plot_defective_density(dataCI,layout=c(2,2))
# Finally lets add in the a bias to respond left by setting the starting point
# of accumulation close to the left than right boundary. We will make a copy of
# the parameter vector and adjust its value but can use the same design. We
# can see z is now closer to 0 than a
p_vectorCIZ <- p_vectorCI.
p_vectorCIZ["Z"] <- qnorm(0.45)
# Finally lets add in the a bias to respond left by setting the starting point
# of accumulation close to the left than right boundary. We will make a copy of
# the parameter vector and adjust its value but can use the same design. We
# can see z is now closer to 0 than a
p_vectorCIZ <- p_vectorCI
p_vectorCIZ["Z"] <- qnorm(0.45)
mapped_par(p_vectorCIZ,designCI)[,c(1:7,9,12,15)]
# We see that the probability of being correct is higher for left than right
# and that correct RT is less for left than right, but the opposite is true for
# the error RT (as an error for a left stimulus is a right response and a right
# stimulus error is a left response), although these effects are more subtle.
dataCIZ <- make_data(p_vectorCIZ,designCI,trials=10000)
plot_defective_density(dataCIZ,layout=c(2,2))
# For now we will use the default unit normal prior which is:
n_pars <- length(p_vectorCIZ)
prior <- list(theta_mu_mean = rep(0, n_pars), theta_mu_var = rep(1, n_pars))
data <- make_data(p_vectorCIZ,designCI,trials=100)
# This creates an object ready to be filled in by sampling with the first
# iteration created by sampling from the prior. By default
# it makes three chains. We use a sampler appropriate for a single subject
# (can also fit a set of participants separately). Later we will look at
# "hierarchical" models that take advantage of commonalities among subjects.
samplers <- make_samplers(dataCIZ,designCI,type="single",rt_resolution=.05,
prior=prior,n_chains=3)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
samples <- run_emc(samplers,cores_per_chain = 2)
prior <- list(theta_mu_mean = rep(0, n_pars), theta_mu_var = diag(rep(1, n_pars)))
# This creates an object ready to be filled in by sampling with the first
# iteration created by sampling from the prior. By default
# it makes three chains. We use a sampler appropriate for a single subject
# (can also fit a set of participants separately). Later we will look at
# "hierarchical" models that take advantage of commonalities among subjects.
samplers <- make_samplers(dataCIZ,designCI,type="single",rt_resolution=.05,
prior=prior,n_chains=3)
prior <- list(theta_mu_mean = rep(0, n_pars), theta_mu_var = diag(rep(1, n_pars)))
data <- make_data(p_vectorCIZ,designCI,trials=100)
# This creates an object ready to be filled in by sampling with the first
# iteration created by sampling from the prior. By default
# it makes three chains. We use a sampler appropriate for a single subject
# (can also fit a set of participants separately). Later we will look at
# "hierarchical" models that take advantage of commonalities among subjects.
samplers <- make_samplers(dataCIZ,designCI,type="single",rt_resolution=.05,
prior=prior,n_chains=3)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
samples <- run_emc(samplers,cores_per_chain = 2)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
samples <- run_emc(samplers,cores_per_chain = 2, verbose = T)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
samples <- auto_burn(samplers,cores_per_chain = 2, verbose = T)
samples <- run_adapt(samples)
debug(run_adapt)
samples <- run_adapt(samples)
debug(run_samplers)
progress <- check_progress(samplers, stage, iter, max_gd,
mean_gd, min_es, min_unique, max_trys, step_size, cores_per_chain,
verbose)
debug(check_progress)
progress <- check_progress(samplers, stage, iter, max_gd,
mean_gd, min_es, min_unique, max_trys, step_size, cores_per_chain,
verbose)
check_gd(samplers, stage, max_gd, mean_gd, trys, verbose)
ifelse(is.null(iter) || length(iter) == 0,
TRUE, total_iters_stage >= iter)
merge_samples(samplers)
extract_samples(samples_merged, stage = "adapt",
samples_merged$samples$idx)
test_adapted(samplers[[1]], test_samples,
min_unique, n_cores, verbose)
debug(test_adapted)
test_adapted(samplers[[1]], test_samples,
min_unique, n_cores, verbose)
test_samples$alpha
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
samples <- run_adapt(samples)
debug(run_adapt)
samples <- run_adapt(samples)
debug(check_progress)
samples <- run_adapt(samples)
iter_done
debug(check_progress)
samples <- run_adapt(samples)
dim(test_samples$alpha)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
debug(check_progress)
samples <- run_adapt(samples)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
debug(check_progress)
samples <- run_adapt(samples)
,
test_samples
dim(test_samples$alpha)
test_adapted(samplers[[1]], test_samples,
min_unique, n_cores, verbose)
samples_merged$samples$alpha
dim(samples_merged$samples$alpha)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
samples <- auto_burn(samplers,cores_per_chain = 2, verbose = T)
samples <- run_adapt(samples)
debug(check_progress)
samples <- run_adapt(samples)
test_samples$alpha
extract_samples(samples_merged, stage = "burn",
samples_merged$samples$idx)
test_samples <- extract_samples(samples_merged, stage = "burn",
samples_merged$samples$idx)
test_samples$alpha
dim(test_samples$alpha)
debug(extract_samples)
test_samples <- extract_samples(samples_merged, stage = "burn",
samples_merged$samples$idx)
debug(variant_funs$filtered_samples)
# This creates an object ready to be filled in by sampling with the first
# iteration created by sampling from the prior. By default
# it makes three chains. We use a sampler appropriate for a single subject
# (can also fit a set of participants separately). Later we will look at
# "hierarchical" models that take advantage of commonalities among subjects.
samplers <- make_samplers(dataCIZ,designCI,type="single",rt_resolution=.05,
prior=prior,n_chains=3)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
samples <- auto_burn(samplers,cores_per_chain = 2, verbose = T)
samples <- run_adapt(samples)
samples <- run_adapt(samples)
samples <- run_adapt(samples, verbose = T)
debug(check_progress)
samples <- run_adapt(samples, verbose = T)
adapted
debug(test_adapted)
samples <- run_adapt(samples, verbose = T)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
debug(test_adapted)
samples <- run_adapt(samples, verbose = T)
n_pars
debug(test_adapted)
samples <- run_adapt(samples, verbose = T, min_unique = 50)
n_unique_sub
first_par
test_samples$alpha
test_samples$alpha[1,, drop = F]
test_samples$alpha[1,,, drop = F]
first_par <- as.matrix(test_samples$alpha[1,,])
first_par
first_par_list <- split(first_par, seq(NROW(first_par)))
n_unique_sub <- lapply(lapply(first_par_list, unique), length)
n_unique_sub
apply(first_par, 2, FUN = function(x) return(length(unique(x))))
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
debug(test_adapted)
samples <- run_adapt(samples, verbose = T, min_unique = 50)
samples <- run_sample(samples, verbose = T)
rm(list=ls())
library(EMC2)
print(load("test_files/PNAS.RData"))
dat <- data[,c("s","E","S","R","RT")]
names(dat)[c(1,5)] <- c("subjects","rt")
levels(dat$R) <- levels(dat$S)
head(dat)
# NB: This data has been truncated at 0.25s and 1.5s
# Average rate = intercept, and rate d = difference (match-mismatch) contrast
ADmat <- matrix(c(-1/2,1/2),ncol=1,dimnames=list(NULL,"d"))
ADmat
Emat <- matrix(c(0,-1,0,0,0,-1),nrow=3)
dimnames(Emat) <- list(NULL,c("a-n","a-s"))
Emat
# Here we fit a series of models
# We'll first build several plausible models and then do model selection
# Only B affected by E
design_B <- make_design(
Ffactors=list(subjects=levels(dat$subjects),S=levels(dat$S),E=levels(dat$E)),
Rlevels=levels(dat$R),matchfun=function(d)d$S==d$lR,
Flist=list(v~lM,sv~1,B~E,A~1,t0~1),
constants=c(sv=log(1)),
model=lbaB)
prior <- list(
theta_mu_mean = 1:5,
theta_mu_var = diag(c(5:1))
) # This way we're using default priors for the nuisance parameters
dat2 <- dat[dat$subjects == unique(dat$subjects)[1:2],]
# Nuisance non hyper = non hierarchically estimated parameters
samplers <- make_samplers(dat2, design_B, type = "single")
samplers <- auto_burn(samplers, verbose = T, cores_for_chains = 3, cores_per_chain = 2)
samplers <- run_adapt(samplers, cores_for_chains = 3, cores_per_chain = 2, verbose = T)
first_par
samplers <- run_adapt(samplers, cores_for_chains = 3, cores_per_chain = 2, verbose = T)
n_unique_sub
length(n_unique_sub)
length(1)
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
debug(test_adapted)
samplers <- run_adapt(samplers, cores_per_chain = 4, verbose = T, min_unique = 100)
''
n_unique_sub
first_par
apply(first_par, 1, FUN = function(x) return(length(unique(x))))
# The speedup factor here is related to rt_resolution, it makes computing
# faster by treating rts in the same 0.05s bin as the same.
devtools::load_all("~/Documents/UVA/2022/EMC2/")
debug(test_adapted)
samples <- run_adapt(samples, verbose = T, min_unique = 50)
debug(test_adapted)
samplers <- run_adapt(samplers, cores_per_chain = 4, verbose = T, min_unique = 100)
n_unique_sub
min_unique
rm(list=ls())
devtools::load_all("~/Documents/UVA/2022/EMC2/")
print(load("test_files/PNAS.RData"))
dat <- data[,c("s","E","S","R","RT")]
names(dat)[c(1,5)] <- c("subjects","rt")
levels(dat$R) <- levels(dat$S)
head(dat)
# Average rate = intercept, and rate d = difference (match-mismatch) contrast
ADmat <- matrix(c(-1/2,1/2),ncol=1,dimnames=list(NULL,"d"))
ADmat
Emat <- matrix(c(0,-1,0,0,0,-1),nrow=3)
dimnames(Emat) <- list(NULL,c("a-n","a-s"))
Emat
# Here we fit a series of models
# We'll first build several plausible models and then do model selection
# Only B affected by E
design_B <- make_design(
Ffactors=list(subjects=levels(dat$subjects),S=levels(dat$S),E=levels(dat$E)),
Rlevels=levels(dat$R),matchfun=function(d)d$S==d$lR,
Flist=list(v~lM,sv~1,B~E,A~1,t0~1),
constants=c(sv=log(1)),
model=lbaB)
prior <- list(
theta_mu_mean = 1:5,
theta_mu_var = diag(c(5:1))
) # This way we're using default priors for the nuisance parameters
dat2 <- dat[dat$subjects == unique(dat$subjects)[1:2],]
dat2
# Nuisance non hyper = non hierarchically estimated parameters
samplers <- make_samplers(dat2, design_B, type = "single")
samplers <- auto_burn(samplers, verbose = T, cores_for_chains = 3, cores_per_chain = 2)
dat2$subjects <- droplevels(dat2$subjects)
# Nuisance non hyper = non hierarchically estimated parameters
samplers <- make_samplers(dat2, design_B, type = "single")
samplers <- auto_burn(samplers, verbose = T, cores_for_chains = 3, cores_per_chain = 2)
samplers <- run_adapt(samplers, cores_for_chains = 3, cores_per_chain = 2, verbose = T, min_unique = 50)
dat2 <- dat[dat$subjects == unique(dat$subjects)[1],]
dat2$subjects <- droplevels(dat2$subjects)
# Nuisance non hyper = non hierarchically estimated parameters
samplers <- make_samplers(dat2, design_B, type = "single")
samplers <- auto_burn(samplers, verbose = T, cores_for_chains = 3, cores_per_chain = 2)
debug(test_adapted)
samplers <- run_adapt(samplers, cores_for_chains = 3, cores_per_chain = 2, verbose = T, min_unique = 50)
n_unique_sub
rm(list=ls())
devtools::load_all("~/Documents/UVA/2022/EMC2/")
print(load("test_files/PNAS.RData"))
dat <- data[,c("s","E","S","R","RT")]
names(dat)[c(1,5)] <- c("subjects","rt")
levels(dat$R) <- levels(dat$S)
head(dat)
# NB: This data has been truncated at 0.25s and 1.5s
# Average rate = intercept, and rate d = difference (match-mismatch) contrast
ADmat <- matrix(c(-1/2,1/2),ncol=1,dimnames=list(NULL,"d"))
ADmat
Emat <- matrix(c(0,-1,0,0,0,-1),nrow=3)
dimnames(Emat) <- list(NULL,c("a-n","a-s"))
Emat
# Here we fit a series of models
# We'll first build several plausible models and then do model selection
# Only B affected by E
design_B <- make_design(
Ffactors=list(subjects=levels(dat$subjects),S=levels(dat$S),E=levels(dat$E)),
Rlevels=levels(dat$R),matchfun=function(d)d$S==d$lR,
Flist=list(v~lM,sv~1,B~E,A~1,t0~1),
constants=c(sv=log(1)),
model=lbaB)
prior <- list(
theta_mu_mean = 1:5,
theta_mu_var = diag(c(5:1))
) # This way we're using default priors for the nuisance parameters
dat2 <- dat[dat$subjects == unique(dat$subjects)[1:4],]
dat2$subjects <- droplevels(dat2$subjects)
# Nuisance non hyper = non hierarchically estimated parameters
samplers <- make_samplers(dat2, design_B, type = "standard")
samplers <- auto_burn(samplers, verbose = T, cores_for_chains = 3, cores_per_chain = 2)
rm(list=ls())
devtools::load_all("~/Documents/UVA/2022/EMC2/")
print(load("test_files/PNAS.RData"))
dat <- data[,c("s","E","S","R","RT")]
names(dat)[c(1,5)] <- c("subjects","rt")
levels(dat$R) <- levels(dat$S)
head(dat)
# NB: This data has been truncated at 0.25s and 1.5s
# Average rate = intercept, and rate d = difference (match-mismatch) contrast
ADmat <- matrix(c(-1/2,1/2),ncol=1,dimnames=list(NULL,"d"))
ADmat
Emat <- matrix(c(0,-1,0,0,0,-1),nrow=3)
dimnames(Emat) <- list(NULL,c("a-n","a-s"))
Emat
# Here we fit a series of models
# We'll first build several plausible models and then do model selection
# Only B affected by E
design_B <- make_design(
Ffactors=list(subjects=levels(dat$subjects),S=levels(dat$S),E=levels(dat$E)),
Rlevels=levels(dat$R),matchfun=function(d)d$S==d$lR,
Flist=list(v~lM,sv~1,B~E,A~1,t0~1),
constants=c(sv=log(1)),
model=lbaB)
prior <- list(
theta_mu_mean = 1:5,
theta_mu_var = diag(c(5:1))
) # This way we're using default priors for the nuisance parameters
dat2 <- dat[dat$subjects == unique(dat$subjects)[1:4],]
rm(list=ls())
devtools::load_all("~/Documents/UVA/2022/EMC2/")
print(load("test_files/PNAS.RData"))
dat <- data[,c("s","E","S","R","RT")]
names(dat)[c(1,5)] <- c("subjects","rt")
levels(dat$R) <- levels(dat$S)
head(dat)
# NB: This data has been truncated at 0.25s and 1.5s
# Average rate = intercept, and rate d = difference (match-mismatch) contrast
ADmat <- matrix(c(-1/2,1/2),ncol=1,dimnames=list(NULL,"d"))
ADmat
Emat <- matrix(c(0,-1,0,0,0,-1),nrow=3)
dimnames(Emat) <- list(NULL,c("a-n","a-s"))
Emat
# Here we fit a series of models
# We'll first build several plausible models and then do model selection
# Only B affected by E
design_B <- make_design(
Ffactors=list(subjects=levels(dat$subjects),S=levels(dat$S),E=levels(dat$E)),
Rlevels=levels(dat$R),matchfun=function(d)d$S==d$lR,
Flist=list(v~lM,sv~1,B~E,A~1,t0~1),
constants=c(sv=log(1)),
model=lbaB)
prior <- list(
theta_mu_mean = 1:5,
theta_mu_var = diag(c(5:1))
) # This way we're using default priors for the nuisance parameters
dat2 <- dat[dat$subjects %in% unique(dat$subjects)[1:4],]
dat2$subjects <- droplevels(dat2$subjects)
# Nuisance non hyper = non hierarchically estimated parameters
samplers <- make_samplers(dat2, design_B, type = "standard")
samplers <- auto_burn(samplers, verbose = T, cores_for_chains = 3, cores_per_chain = 2)
debug(test_adapted)
samplers <- run_adapt(samplers, cores_for_chains = 3, cores_per_chain = 2, verbose = T, min_unique = 50)
devtools::load_all("~/Documents/UVA/2022/EMC2/")
devtools::load_all("~/Documents/UVA/2022/EMC2/")
# samplers <- run_emc(samplers, cores_per_chain = 5, cores_for_chains = 1, verbose = T)
?DDMtzd
# samplers <- run_emc(samplers, cores_per_chain = 5, cores_for_chains = 1, verbose = T)
?run_sample
q()
devtools::load_all("~/Documents/UVA/2022/EMC2/")
?make_samplers
remove.packages("EMC2")
devtools::install("~/Documents/UVA/2022/EMC2/")
library(EMC2)
?run_sample
library(EMC2)
?make_samplers
?ddmTZD
devtools::document()
devtools::load_all()
?ddmTZD
remove.packages("EMC2")
devtools::install()
?ddmTZD
?make_samplers
q()
